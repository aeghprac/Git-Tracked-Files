{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-14 16:28:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2023-01-14 16:28:35 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  5 2022, 01:53:17) - [Clang 12.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2023-01-14 16:28:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-01-14 16:28:35 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-01-14 16:28:35 [scrapy.extensions.telnet] INFO: Telnet Password: 44caed9a92bdc55e\n",
      "2023-01-14 16:28:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-01-14 16:28:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-01-14 16:28:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-01-14 16:28:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-01-14 16:28:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-01-14 16:28:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Attempting to acquire lock 140449333700736 on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Lock 140449333700736 acquired on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Attempting to acquire lock 140449333702320 on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Lock 140449333702320 acquired on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Attempting to release lock 140449333702320 on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Lock 140449333702320 released on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Attempting to release lock 140449333700736 on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [filelock] DEBUG: Lock 140449333700736 released on /Users/aepperly/.cache/python-tldextract/3.9.12.final__anaconda3__768093__tldextract-3.2.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2023-01-14 16:28:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2023-01-14 16:28:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-01-14 16:28:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 7068,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2464272,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 0.634515,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 1, 14, 21, 28, 36, 105962),\n",
      " 'httpcompression/response_bytes': 2051,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/404': 1,\n",
      " 'log_count/DEBUG': 21,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 100196352,\n",
      " 'memusage/startup': 100196352,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2023, 1, 14, 21, 28, 35, 471447)}\n",
      "2023-01-14 16:28:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Intermediate R - Practice': 'This follow-up course on Intermediate R does not cover new programming concepts. Instead, you will strengthen your knowledge of the topics in Intermediate R with a bunch of new and fun exercises.', 'Intermediate R': 'The intermediate R course is the logical next stop on your journey in the R programming language. In this R training you will learn about conditional statements, loops and functions to power your own R scripts. Next, you can make your R code more efficient and readable using the apply functions. Finally, the utilities chapter gets you up to speed with regular expressions in the R programming language, data structure manipulations and times and dates. This R tutorial will allow you to learn R and take the next step in advancing your overall knowledge and capabilities while programming in R.', 'Data Visualization in R with ggvis': 'Learn to create interactive graphs to display distributions, relationships, model fits, and more using ggvis.', 'Intro to Python for Data Science': 'Python is a general-purpose programming language that is becoming more and more popular for doing data science. Companies worldwide are using Python to harvest insights from their data and get a competitive edge. Unlike any other Python tutorial, this course focuses on Python specifically for data science. In our Intro to Python class, you will learn about powerful ways to store and manipulate data as well as cool data science tools to start your own analyses. Enter DataCamp’s online Python curriculum.', 'Data Analysis in R, the data.table Way': 'The R data.table package is rapidly making its name as the number one choice for handling large datasets in R. This online data.table tutorial will bring you from data.table novice to expert in no time. Once you are introduced to the general form of a data.table query, you will learn the techniques to subset your data.table, how to update by reference and how you can use data.table’s set()-family in your workflow. The course finishes with more complex concepts such as indexing, keys and fast ordered joins. Upon completion of the course, you will be able to use data.table in R for a more efficient manipulation and analysis process. Enjoy!', 'Reporting with R Markdown': 'Learn how to write a data report quickly and effectively with the R Markdown package, and share your results with your friends, colleagues or the rest of the world.  Learn how you can author your own R Markdown reports, and how to automate the reporting process so that you have your own reproducible reports. By the end of the interactive data analysis reporting tutorial, you will be able to generate reports straight from your R code, documenting your work — and its results — as an HTML, pdf, slideshow or Microsoft Word document.', 'Data Manipulation in R with dplyr': 'In this interactive tutorial, you will learn how to perform sophisticated dplyr techniques to carry out your data manipulation with R. First you will master the five verbs of R data manipulation with dplyr: select, mutate, filter, arrange and summarise. Next, you will learn how you can chain your dplyr operations using the pipe operator of the magrittr package. In the final section, the focus is on practicing how to subset your data using the group_by function, and how you can access data stored outside of R in a database. All said and done, you will be familiar with data manipulation tools and techniques that will allow you to efficiently manipulate data.', 'Cleaning Data in R': \"It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time actually analyzing it. For this reason, it is critical to become familiar with the data cleaning process and all of the tools available to you along the way. This course provides a very basic introduction to cleaning data in R using the tidyr, dplyr, and stringr packages. After taking the course you'll be able to go from raw data to awesome insights as quickly and painlessly as possible!\", 'Introduction to Machine Learning': \"This online machine learning course is perfect for those who have a solid basis in R and statistics, but are complete beginners with machine learning. After a broad overview of the discipline's most common techniques and applications, you'll gain more insight into the assessment and training of different machine learning models. The rest of the course is dedicated to a first reconnaissance with three of the most basic machine learning tasks: classification, regression and clustering.\", 'Introduction to R': 'In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!'}\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css('p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "print(dc_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba4959ed769923c7c7779cf615c3f4552d8348c3c788a158f96a2a3da27ac90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
